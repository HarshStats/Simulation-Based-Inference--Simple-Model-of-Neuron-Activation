{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff7890a",
   "metadata": {},
   "source": [
    "# Phase 4: Neural Posterior Training with BayesFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ae12e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**Goal:**  \n",
    "Train a neural posterior using BayesFlow for the LIF neuron model, leveraging the prior and simulator defined in previous phases. This phase demonstrates the full SBI workflow: training, inference, and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34453774",
   "metadata": {},
   "source": [
    "## 2. BayesFlow Workflow Recap\n",
    "\n",
    "- **Prior:** Samples plausible parameters.\n",
    "- **Simulator:** Generates simulated data given parameters.\n",
    "- **AmortizedPosterior:** Neural network that learns to approximate the posterior $p(\\theta|x)$.\n",
    "- **Trainer:** Handles simulation-based training using the prior and simulator.\n",
    "\n",
    "BayesFlow trains the neural posterior by drawing fresh parameter/data pairs on-the-fly, ensuring robust and generalizable inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36b8ed",
   "metadata": {},
   "source": [
    "## 3. Setup: Imports and Functions\n",
    "\n",
    "Make sure BayesFlow is installed:\n",
    "```bash\n",
    "pip install bayesflow\n",
    "```\n",
    "\n",
    "Import BayesFlow and bring in your prior and simulator from Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537d8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Using backend 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import numpy as np\n",
    "import bayesflow as bf\n",
    "from src.lif_model import lif_simulate\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Parameter bounds (should match previous phases)\n",
    "PARAM_BOUNDS = {\n",
    "    'tau_m':   (5.0, 30.0),    # ms\n",
    "    'E_L':     (-80.0, -65.0), # mV\n",
    "    'g_L':     (5.0, 15.0),    # nS\n",
    "    'V_th':    (-60.0, -50.0), # mV\n",
    "    'V_reset': (-80.0, -65.0), # mV\n",
    "    'I':       (50.0, 300.0),  # pA\n",
    "}\n",
    "PARAM_KEYS = list(PARAM_BOUNDS.keys())\n",
    "\n",
    "# Prior function\n",
    "def prior(batch_size):\n",
    "    samples = []\n",
    "    for key in PARAM_KEYS:\n",
    "        low, high = PARAM_BOUNDS[key]\n",
    "        samples.append(uniform.rvs(loc=low, scale=high-low, size=batch_size))\n",
    "    return np.stack(samples, axis=1)\n",
    "\n",
    "# Simulator function\n",
    "def simulator(params, T=200.0, dt=0.1, tref=2.0, noise_std=0.5):\n",
    "    traces = []\n",
    "    for p in params:\n",
    "        param_dict = dict(zip(PARAM_KEYS, p))\n",
    "        sim_args = {\n",
    "            'T': T,\n",
    "            'dt': dt,\n",
    "            'E_L': param_dict['E_L'],\n",
    "            'V_th': param_dict['V_th'],\n",
    "            'V_reset': param_dict['V_reset'],\n",
    "            'tau_m': param_dict['tau_m'],\n",
    "            'g_L': param_dict['g_L'],\n",
    "            'I': param_dict['I'],\n",
    "            'tref': tref\n",
    "        }\n",
    "        t, V, spikes = lif_simulate(**sim_args)\n",
    "        V_noisy = V + np.random.normal(0, noise_std, size=V.shape)\n",
    "        traces.append(V_noisy)\n",
    "    return np.stack(traces, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2986a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator class for BayesFlow v2+\n",
    "class LIFSimulator:\n",
    "    def __init__(self, param_keys, noise_std=0.5):\n",
    "        self.param_keys = param_keys\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        params = prior(batch_size)\n",
    "        traces = []\n",
    "        param_dicts = {key: [] for key in self.param_keys}\n",
    "        for p in params:\n",
    "            param_dict = dict(zip(self.param_keys, p))\n",
    "            sim_args = {\n",
    "                'T': 200.0,\n",
    "                'dt': 0.1,\n",
    "                'E_L': param_dict['E_L'],\n",
    "                'V_th': param_dict['V_th'],\n",
    "                'V_reset': param_dict['V_reset'],\n",
    "                'tau_m': param_dict['tau_m'],\n",
    "                'g_L': param_dict['g_L'],\n",
    "                'I': param_dict['I'],\n",
    "                'tref': 2.0\n",
    "            }\n",
    "            t, V, spikes = lif_simulate(**sim_args)\n",
    "            V_noisy = V + np.random.normal(0, self.noise_std, size=V.shape)\n",
    "            traces.append(V_noisy)\n",
    "            for key in self.param_keys:\n",
    "                param_dicts[key].append(param_dict[key])\n",
    "        # Convert lists to arrays\n",
    "        for key in param_dicts:\n",
    "            param_dicts[key] = np.array(param_dicts[key]).reshape(-1, 1)\n",
    "        out = {**param_dicts, \"trace\": np.stack(traces, axis=0)}\n",
    "        return out\n",
    "\n",
    "# Instantiate the simulator object\n",
    "simulator = LIFSimulator(PARAM_KEYS)\n",
    "\n",
    "# Redefine the workflow to use the simulator object\n",
    "workflow = bf.BasicWorkflow(\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    "    inference_variables=PARAM_KEYS,\n",
    "    summary_variables=[\"trace\"],\n",
    "    simulator=simulator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b5c7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator output keys: ['parameters', 'trace']\n",
      "parameters: shape (1, 6)\n",
      "trace: shape (1, 2000)\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check simulator output keys and shapes\n",
    "sim_out = simulator.sample(1)\n",
    "print(\"Simulator output keys:\", list(sim_out.keys()))\n",
    "for k, v in sim_out.items():\n",
    "    print(f\"{k}: shape {v.shape}\")\n",
    "# Should see all parameter names and 'trace' as keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c22c38",
   "metadata": {},
   "source": [
    "## 4. Define and Initialize the Neural Posterior\n",
    "\n",
    "We use BayesFlow's `AmortizedPosterior` to learn the mapping from simulated traces to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f0b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BayesFlow v2+ BasicWorkflow for amortized inference\n",
    "# Define summary and inference networks\n",
    "summary_network = bf.networks.TimeSeriesNetwork()\n",
    "inference_network = bf.networks.CouplingFlow()\n",
    "\n",
    "# Define the workflow\n",
    "workflow = bf.BasicWorkflow(\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    "    inference_variables=PARAM_KEYS,\n",
    "    summary_variables=[\"trace\"],  # adjust as needed for your simulator output\n",
    "    simulator=simulator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb1f23",
   "metadata": {},
   "source": [
    "## 5. Training the Posterior with BayesFlow\n",
    "\n",
    "The `workflow.fit_online(...)` method in BayesFlow v2+ handles simulation-based training. It repeatedly samples parameters from the prior, simulates data using your LIF model, and updates the neural posterior. This approach enables efficient amortized inference and parameter recovery for new simulated trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3414977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Missing keys: {'V_reset', 'V_th', 'E_L', 'tau_m', 'g_L', 'I'}\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m n_epochs = \u001b[32m10\u001b[39m  \u001b[38;5;66;03m# Increase for real use\u001b[39;00m\n\u001b[32m      5\u001b[39m n_simulations_per_epoch = \u001b[32m128\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m history = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_simulations_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:784\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m dataset = OnlineDataset(\n\u001b[32m    777\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    778\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     augmentations=augmentations,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:954\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:316\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/approximators/approximator.py:134\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.built:\n\u001b[32m    133\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mBuilding on a test batch.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     mock_data = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    135\u001b[39m     mock_data = keras.tree.map_structure(keras.ops.convert_to_tensor, mock_data)\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/datasets/online_dataset.py:94\u001b[39m, in \u001b[36mOnlineDataset.__getitem__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not apply augmentations of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.augmentations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/adapters/adapter.py:182\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, data, inverse, stage, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inverse(data, stage=stage, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/adapters/adapter.py:113\u001b[39m, in \u001b[36mAdapter.forward\u001b[39m\u001b[34m(self, data, stage, log_det_jac, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m log_det_jac:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         data = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m    116\u001b[39m log_det_jac = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/adapters/transforms/transform.py:16\u001b[39m, in \u001b[36mTransform.__call__\u001b[39m\u001b[34m(self, data, inverse, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inverse(data, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Simulation-Based-Inference--Simple-Model-of-Neuron-Activation/.venv/lib/python3.12/site-packages/bayesflow/adapters/transforms/concatenate.py:65\u001b[39m, in \u001b[36mConcatenate.forward\u001b[39m\u001b[34m(self, data, strict, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m missing_keys = required_keys - available_keys\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m missing_keys:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# invalid call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m missing_keys:\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# we cannot produce a result, but should still remove the keys\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m common_keys:\n",
      "\u001b[31mKeyError\u001b[39m: \"Missing keys: {'V_reset', 'V_th', 'E_L', 'tau_m', 'g_L', 'I'}\""
     ]
    }
   ],
   "source": [
    "# Train the neural posterior with BayesFlow v2+\n",
    "# This cell assumes the simulator and workflow are defined as above.\n",
    "\n",
    "n_epochs = 10  # Increase for real use\n",
    "n_simulations_per_epoch = 128\n",
    "\n",
    "history = workflow.fit_online(\n",
    "    epochs=n_epochs,\n",
    "    batch_size=n_simulations_per_epoch,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0099",
   "metadata": {},
   "source": [
    "## 6. Inference: Posterior Sampling\n",
    "\n",
    "After training, you can use the neural posterior to infer parameters from new (possibly real) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409e1ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LIFSimulator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: infer parameters for a new simulated trace\u001b[39;00m\n\u001b[32m      2\u001b[39m true_params = prior(\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m obs = \u001b[43msimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# BayesFlow expects a dict with summary_variables as keys\u001b[39;00m\n\u001b[32m      6\u001b[39m obs_dict = {\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m: obs}\n",
      "\u001b[31mTypeError\u001b[39m: 'LIFSimulator' object is not callable"
     ]
    }
   ],
   "source": [
    "# Parameter recovery: simulate new data, run inference, and compare posterior samples to true parameters\n",
    "\n",
    "# Simulate new parameters and data\n",
    "n_test = 1\n",
    "sim_out = simulator.sample(n_test)\n",
    "obs_dict = {\"trace\": sim_out[\"trace\"]}\n",
    "true_params = np.array([sim_out[key][0, 0] for key in PARAM_KEYS])\n",
    "\n",
    "# Run inference\n",
    "posterior_samples = workflow.posterior.sample(\n",
    "    conditions=obs_dict,\n",
    "    n_samples=1000\n",
    ")\n",
    "\n",
    "# Plot posterior samples vs. true parameters\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, len(PARAM_KEYS), figsize=(3*len(PARAM_KEYS), 3))\n",
    "for i, key in enumerate(PARAM_KEYS):\n",
    "    ax = axes[i] if len(PARAM_KEYS) > 1 else axes\n",
    "    ax.hist(posterior_samples[key].flatten(), bins=30, alpha=0.7, label='Posterior')\n",
    "    ax.axvline(true_params[i], color='r', linestyle='--', label='True')\n",
    "    ax.set_title(key)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9bfb6",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Parameter Recovery\n",
    "\n",
    "Let's visualize the true parameters vs. the posterior samples for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "for i, key in enumerate(PARAM_KEYS):\n",
    "    axes[i].hist(posterior_samples[:, i], bins=30, color='lightgreen', edgecolor='k', alpha=0.7)\n",
    "    axes[i].axvline(true_params[0, i], color='r', linestyle='--', label='True')\n",
    "    axes[i].set_title(key)\n",
    "    axes[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afd3c6",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps\n",
    "\n",
    "- You have trained a BayesFlow neural posterior for the LIF neuron model.\n",
    "- You can now use it for inference and parameter recovery on new data.\n",
    "- For best results, increase the number of training iterations and tune the network as needed.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
